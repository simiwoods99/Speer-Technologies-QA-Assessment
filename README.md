## ðŸ•·ï¸ Wikipedia Spider Crawler
Author : Samir Khiri
Date : Wed Jun 25, 2025

A lightweight **Node.js script** that crawls Wikipedia articles starting from a user-specified page, exploring up to `n` link-depths (1â€“3), and writing all visited article URLs to a CSV file.

---

### ðŸ“Œ Features

* âœ… Starts from any valid Wikipedia article
* âœ… Traverses up to 10 unique, valid article links per page
* âœ… Depth-limited crawl (user-specified: 1â€“3)
* âœ… Skips non-article links (e.g., File:, Talk:, Special:)
* âœ… Outputs visited pages to `visited_pages.csv`

---

### ðŸ§° Requirements

* Node.js v14+
* Internet connection

---

### ðŸ“¦ Installation

```bash
git clone https://github.com/simiwoods99/wiki-spider.git
cd wiki-spider
npm install
```

---

### â–¶ï¸ Usage

```bash
node index.js
```

You will be prompted to:

* Enter a valid Wikipedia article URL (e.g. `https://en.wikipedia.org/wiki/Computer`)
* Enter crawl depth (1â€“3)

---

### ðŸ“ Output

Creates a `visited_pages.csv` with this format:

```csv
Index,URL
1,"https://en.wikipedia.org/wiki/Computer"
2,"https://en.wikipedia.org/wiki/Technology"
3,"https://en.wikipedia.org/wiki/Science"
...
```

---

### ðŸ“’ Assumptions

* Only links within the **main content area** (`#bodyContent`) are crawled.
* Internal page anchors, media, and special pages are ignored.
* If fewer than 10 links are available, all valid ones are followed.

---

### ðŸ§ª Example Run

```bash
Enter your wikipedia page of interest to spider! ðŸ•·ï¸ 
> https://en.wikipedia.org/wiki/Computer
Enter your desired depth (1-3): 
> 2
âœ… Spider completed! CSV written to visited_pages.csv
```

